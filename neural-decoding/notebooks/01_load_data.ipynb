{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Loading Data for Neural Decoding\n",
    "\n",
    "This notebook demonstrates how to load pre-processed neuroimaging data\n",
    "for classification-based decoding analysis.\n",
    "\n",
    "**Contents:**\n",
    "1. Loading fMRI data (NIfTI)\n",
    "2. Loading EEG data (MNE Epochs)\n",
    "3. Loading behavioral data (CSV)\n",
    "4. Multimodal data fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Neural decoding imports\n",
    "from io.loaders import FMRILoader, EEGLoader, BehaviorLoader, MultimodalLoader\n",
    "from core.dataset import DecodingDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading fMRI Data\n",
    "\n",
    "For fMRI, you need:\n",
    "- Pre-processed 4D NIfTI file (one volume per trial/timepoint)\n",
    "- Brain mask (binary NIfTI)\n",
    "- Events file (CSV) with trial labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load fMRI data\n",
    "# (Replace paths with your actual data)\n",
    "\n",
    "fmri_loader = FMRILoader()\n",
    "\n",
    "# Paths to your data\n",
    "data_path = \"sub-01_task_bold.nii.gz\"      # 4D NIfTI\n",
    "mask_path = \"brain_mask.nii.gz\"            # Binary mask\n",
    "events_path = \"events.csv\"                 # Trial events\n",
    "\n",
    "# Load dataset\n",
    "# fmri_dataset = fmri_loader.load(\n",
    "#     data_path=data_path,\n",
    "#     mask_path=mask_path,\n",
    "#     events_path=events_path,\n",
    "#     label_column=\"condition\",  # Column with class labels\n",
    "#     run_column=\"run\",          # Column for CV grouping\n",
    "#     standardize=True\n",
    "# )\n",
    "\n",
    "# print(f\"Loaded fMRI data:\")\n",
    "# print(f\"  Samples: {fmri_dataset.n_samples}\")\n",
    "# print(f\"  Features (voxels): {fmri_dataset.n_features}\")\n",
    "# print(f\"  Classes: {fmri_dataset.class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Events File Format\n",
    "\n",
    "```csv\n",
    "trial,onset,duration,condition,run\n",
    "1,0.0,2.0,face,1\n",
    "2,4.0,2.0,house,1\n",
    "3,8.0,2.0,face,1\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI-Based Loading\n",
    "\n",
    "For ROI-based analysis, use an atlas to extract mean signals per region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load fMRI with ROI features\n",
    "\n",
    "# roi_dataset = fmri_loader.load_roi(\n",
    "#     data_path=\"sub-01_task_bold.nii.gz\",\n",
    "#     atlas_path=\"harvard_oxford.nii.gz\",  # Atlas with integer labels\n",
    "#     events_path=\"events.csv\",\n",
    "#     label_column=\"condition\",\n",
    "#     run_column=\"run\",\n",
    "#     aggregation=\"mean\"  # \"mean\", \"median\", or \"std\"\n",
    "# )\n",
    "\n",
    "# print(f\"ROI features: {roi_dataset.n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading EEG Data\n",
    "\n",
    "For EEG, load MNE Epochs files (.fif format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load EEG epochs\n",
    "\n",
    "eeg_loader = EEGLoader()\n",
    "\n",
    "# Load epochs file\n",
    "# eeg_dataset = eeg_loader.load(\n",
    "#     epochs_path=\"sub-01-epo.fif\",\n",
    "#     time_window=(0.1, 0.5),  # Focus on 100-500ms\n",
    "#     channels=None,           # All channels (or list of names)\n",
    "#     flatten=True             # Flatten channels x time to 1D\n",
    "# )\n",
    "\n",
    "# print(f\"EEG data:\")\n",
    "# print(f\"  Samples: {eeg_dataset.n_samples}\")\n",
    "# print(f\"  Features: {eeg_dataset.n_features}\")\n",
    "# print(f\"  Classes: {eeg_dataset.class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Resolved Loading\n",
    "\n",
    "For temporal decoding, load multiple time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load for time-resolved analysis\n",
    "\n",
    "# datasets_by_time = eeg_loader.load_time_resolved(\n",
    "#     epochs_path=\"sub-01-epo.fif\",\n",
    "#     time_points=np.arange(-0.1, 0.6, 0.05),  # Every 50ms\n",
    "#     window_size=0.05  # 50ms windows\n",
    "# )\n",
    "\n",
    "# print(f\"Created {len(datasets_by_time)} time-point datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Behavioral Data\n",
    "\n",
    "For behavioral-only decoding or to add behavioral features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load behavioral data\n",
    "\n",
    "behavior_loader = BehaviorLoader()\n",
    "\n",
    "# behavior_dataset = behavior_loader.load(\n",
    "#     csv_path=\"behavior.csv\",\n",
    "#     feature_columns=[\"reaction_time\", \"accuracy\", \"confidence\"],\n",
    "#     label_column=\"condition\",\n",
    "#     group_column=\"subject\",  # For CV grouping\n",
    "#     standardize=True\n",
    "# )\n",
    "\n",
    "# print(f\"Behavioral features: {behavior_dataset.feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multimodal Data Fusion\n",
    "\n",
    "Combine features from multiple modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Early fusion (feature concatenation)\n",
    "\n",
    "multimodal_loader = MultimodalLoader()\n",
    "\n",
    "# Combine fMRI, EEG, and behavior\n",
    "# fused_dataset = multimodal_loader.early_fusion(\n",
    "#     datasets=[fmri_dataset, eeg_dataset, behavior_dataset],\n",
    "#     normalize=True  # Z-score each modality before fusion\n",
    "# )\n",
    "\n",
    "# print(f\"Fused features: {fused_dataset.n_features}\")\n",
    "# print(f\"Modalities: {fused_dataset.metadata['modalities']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating Synthetic Data for Testing\n",
    "\n",
    "Generate synthetic data to test the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic fMRI-like data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate classification data\n",
    "X, y = make_classification(\n",
    "    n_samples=100,\n",
    "    n_features=1000,  # ~voxels\n",
    "    n_informative=50,\n",
    "    n_redundant=50,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create groups (runs)\n",
    "groups = np.repeat(np.arange(1, 6), 20)  # 5 runs, 20 trials each\n",
    "\n",
    "# Create DecodingDataset\n",
    "synthetic_dataset = DecodingDataset(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    groups=groups,\n",
    "    feature_names=[f\"voxel_{i}\" for i in range(1000)],\n",
    "    class_names=[\"class_A\", \"class_B\"],\n",
    "    metadata={\"synthetic\": True},\n",
    "    modality=\"fmri\"\n",
    ")\n",
    "\n",
    "print(f\"Synthetic dataset created:\")\n",
    "print(f\"  Shape: {synthetic_dataset.X.shape}\")\n",
    "print(f\"  Classes: {synthetic_dataset.class_names}\")\n",
    "print(f\"  Groups: {np.unique(synthetic_dataset.groups)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inspecting DecodingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset properties\n",
    "print(f\"Number of samples: {synthetic_dataset.n_samples}\")\n",
    "print(f\"Number of features: {synthetic_dataset.n_features}\")\n",
    "print(f\"Number of classes: {synthetic_dataset.n_classes}\")\n",
    "print(f\"Class counts: {synthetic_dataset.class_counts}\")\n",
    "print(f\"Is balanced: {synthetic_dataset.is_balanced}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subset by class\n",
    "class_a = synthetic_dataset.get_subset(class_labels=[0])\n",
    "print(f\"Class A samples: {class_a.n_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by group\n",
    "train_data, test_data = synthetic_dataset.split_by_group(test_groups=[5])\n",
    "print(f\"Train samples: {train_data.n_samples}\")\n",
    "print(f\"Test samples: {test_data.n_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to sklearn format\n",
    "X_sk, y_sk, groups_sk = synthetic_dataset.to_sklearn()\n",
    "print(f\"Sklearn arrays: X={X_sk.shape}, y={y_sk.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **02_extract_features.ipynb**: Feature extraction and selection\n",
    "- **03_train_classifier.ipynb**: Training decoders\n",
    "- **04_cross_validation.ipynb**: Cross-validation strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
