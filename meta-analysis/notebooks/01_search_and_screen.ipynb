{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Search and Screen Papers\n",
    "\n",
    "This notebook guides you through:\n",
    "1. Defining your research question and inclusion criteria\n",
    "2. **Searching multiple databases** (PubMed, Google Scholar, Semantic Scholar)\n",
    "3. AI-assisted abstract screening\n",
    "4. Tracking decisions for PRISMA reporting\n",
    "\n",
    "## Supported Search Engines\n",
    "- **PubMed** - Free, requires email only\n",
    "- **Google Scholar** - Via SerpAPI (requires API key) or scholarly (free, rate-limited)\n",
    "- **Semantic Scholar** - Free API with optional key for higher limits\n",
    "- **Scopus** - Requires institutional API key\n",
    "- **Web of Science** - Requires institutional access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - add parent directory to path\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Optional, Any\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration: API Keys & Credentials\n",
    "\n",
    "Set your credentials here or use environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SearchCredentials:\n",
    "    \"\"\"Store credentials for various search engines.\"\"\"\n",
    "    \n",
    "    # PubMed (required: email)\n",
    "    pubmed_email: str = \"\"\n",
    "    pubmed_api_key: Optional[str] = None  # Optional, increases rate limit\n",
    "    \n",
    "    # Google Scholar via SerpAPI\n",
    "    serpapi_key: Optional[str] = None\n",
    "    \n",
    "    # Semantic Scholar\n",
    "    semantic_scholar_key: Optional[str] = None\n",
    "    \n",
    "    # Scopus (Elsevier)\n",
    "    scopus_api_key: Optional[str] = None\n",
    "    scopus_inst_token: Optional[str] = None\n",
    "    \n",
    "    # Institutional proxy (e.g., Rutgers)\n",
    "    proxy_url: Optional[str] = None\n",
    "    proxy_username: Optional[str] = None\n",
    "    proxy_password: Optional[str] = None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_env(cls):\n",
    "        \"\"\"Load credentials from environment variables.\"\"\"\n",
    "        return cls(\n",
    "            pubmed_email=os.getenv(\"PUBMED_EMAIL\", \"\"),\n",
    "            pubmed_api_key=os.getenv(\"PUBMED_API_KEY\"),\n",
    "            serpapi_key=os.getenv(\"SERPAPI_KEY\"),\n",
    "            semantic_scholar_key=os.getenv(\"SEMANTIC_SCHOLAR_KEY\"),\n",
    "            scopus_api_key=os.getenv(\"SCOPUS_API_KEY\"),\n",
    "            scopus_inst_token=os.getenv(\"SCOPUS_INST_TOKEN\"),\n",
    "            proxy_url=os.getenv(\"LIBRARY_PROXY_URL\"),\n",
    "            proxy_username=os.getenv(\"LIBRARY_PROXY_USER\"),\n",
    "            proxy_password=os.getenv(\"LIBRARY_PROXY_PASS\")\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, path: str):\n",
    "        \"\"\"Load credentials from JSON file.\"\"\"\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        return cls(**data)\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save credentials to JSON file (be careful with secrets!).\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(asdict(self), f, indent=2)\n",
    "\n",
    "\n",
    "# Load credentials - EDIT THIS SECTION\n",
    "# Option 1: Set directly\n",
    "credentials = SearchCredentials(\n",
    "    pubmed_email=\"your-email@example.com\",  # REQUIRED for PubMed\n",
    "    # pubmed_api_key=\"your-key\",            # Optional: higher rate limits\n",
    "    # serpapi_key=\"your-serpapi-key\",       # For Google Scholar\n",
    "    # semantic_scholar_key=\"your-s2-key\",   # Optional: higher limits\n",
    "    \n",
    "    # Rutgers proxy example\n",
    "    proxy_url=\"https://www.libraries.rutgers.edu/proxy\",\n",
    "    proxy_username=\"jss388\",  # Your NetID\n",
    ")\n",
    "\n",
    "# Option 2: Load from environment\n",
    "# credentials = SearchCredentials.from_env()\n",
    "\n",
    "# Option 3: Load from file (don't commit this file!)\n",
    "# credentials = SearchCredentials.from_file(\"~/.meta_analysis_credentials.json\")\n",
    "\n",
    "print(\"Credentials configured:\")\n",
    "print(f\"  PubMed email: {'✓' if credentials.pubmed_email else '✗'}\")\n",
    "print(f\"  PubMed API key: {'✓' if credentials.pubmed_api_key else '✗ (optional)'}\")\n",
    "print(f\"  SerpAPI (Google Scholar): {'✓' if credentials.serpapi_key else '✗'}\")\n",
    "print(f\"  Semantic Scholar key: {'✓' if credentials.semantic_scholar_key else '✗ (optional)'}\")\n",
    "print(f\"  Scopus API: {'✓' if credentials.scopus_api_key else '✗'}\")\n",
    "print(f\"  Library proxy: {'✓' if credentials.proxy_url else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Your Research Question\n",
    "\n",
    "Use the PICO framework:\n",
    "- **P**opulation: Who are you studying?\n",
    "- **I**ntervention: What exposure/treatment?\n",
    "- **C**omparison: What is the control condition?\n",
    "- **O**utcome: What are you measuring?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your research question\n",
    "RESEARCH_QUESTION = \"What brain regions are activated during spatial decision-making in T-maze tasks?\"\n",
    "\n",
    "PICO = {\n",
    "    \"Population\": \"Healthy adult humans\",\n",
    "    \"Intervention\": \"T-maze or spatial decision-making task\",\n",
    "    \"Comparison\": \"Control condition or baseline\",\n",
    "    \"Outcome\": \"Brain activation (fMRI coordinates)\"\n",
    "}\n",
    "\n",
    "print(\"Research Question:\")\n",
    "print(f\"  {RESEARCH_QUESTION}\")\n",
    "print(\"\\nPICO:\")\n",
    "for key, value in PICO.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Inclusion/Exclusion Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUSION_CRITERIA = [\n",
    "    \"Reports original fMRI or PET neuroimaging data\",\n",
    "    \"Uses T-maze, spatial navigation, or decision-making task\",\n",
    "    \"Reports activation coordinates in MNI or Talairach space\",\n",
    "    \"Published in peer-reviewed journal\",\n",
    "    \"Human participants\",\n",
    "    \"Written in English\"\n",
    "]\n",
    "\n",
    "EXCLUSION_CRITERIA = [\n",
    "    \"Review articles or meta-analyses without original data\",\n",
    "    \"Case studies with n < 5\",\n",
    "    \"Only ROI analysis (no whole-brain coordinates)\",\n",
    "    \"Clinical populations only (unless healthy control group)\",\n",
    "    \"Animal studies\"\n",
    "]\n",
    "\n",
    "print(\"Inclusion Criteria:\")\n",
    "for i, c in enumerate(INCLUSION_CRITERIA, 1):\n",
    "    print(f\"  {i}. {c}\")\n",
    "\n",
    "print(\"\\nExclusion Criteria:\")\n",
    "for i, c in enumerate(EXCLUSION_CRITERIA, 1):\n",
    "    print(f\"  {i}. {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Search Engine Framework\n",
    "\n",
    "Unified interface for multiple search engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Paper:\n",
    "    \"\"\"Standardized paper representation across all search engines.\"\"\"\n",
    "    title: str\n",
    "    authors: List[str]\n",
    "    year: Optional[int]\n",
    "    abstract: str\n",
    "    source: str  # Which search engine found this\n",
    "    \n",
    "    # Identifiers (may vary by source)\n",
    "    doi: Optional[str] = None\n",
    "    pmid: Optional[str] = None\n",
    "    arxiv_id: Optional[str] = None\n",
    "    semantic_scholar_id: Optional[str] = None\n",
    "    \n",
    "    # Metadata\n",
    "    journal: Optional[str] = None\n",
    "    url: Optional[str] = None\n",
    "    pdf_url: Optional[str] = None\n",
    "    citation_count: Optional[int] = None\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return asdict(self)\n",
    "    \n",
    "    @property\n",
    "    def citation(self) -> str:\n",
    "        author_str = self.authors[0] if self.authors else \"Unknown\"\n",
    "        if len(self.authors) > 1:\n",
    "            author_str += \" et al.\"\n",
    "        return f\"{author_str} ({self.year or 'n.d.'})\"\n",
    "\n",
    "\n",
    "class SearchEngine(ABC):\n",
    "    \"\"\"Abstract base class for search engines.\"\"\"\n",
    "    \n",
    "    name: str = \"BaseEngine\"\n",
    "    \n",
    "    def __init__(self, credentials: SearchCredentials):\n",
    "        self.credentials = credentials\n",
    "    \n",
    "    @abstractmethod\n",
    "    def search(self, query: str, max_results: int = 100) -> List[Paper]:\n",
    "        \"\"\"Search for papers matching query.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def is_available(self) -> bool:\n",
    "        \"\"\"Check if this engine is properly configured.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "print(\"Search engine framework loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PubMedEngine(SearchEngine):\n",
    "    \"\"\"Search PubMed via Entrez API.\"\"\"\n",
    "    \n",
    "    name = \"PubMed\"\n",
    "    \n",
    "    def is_available(self) -> bool:\n",
    "        try:\n",
    "            from Bio import Entrez\n",
    "            return bool(self.credentials.pubmed_email)\n",
    "        except ImportError:\n",
    "            return False\n",
    "    \n",
    "    def search(self, query: str, max_results: int = 100) -> List[Paper]:\n",
    "        from Bio import Entrez\n",
    "        \n",
    "        Entrez.email = self.credentials.pubmed_email\n",
    "        if self.credentials.pubmed_api_key:\n",
    "            Entrez.api_key = self.credentials.pubmed_api_key\n",
    "        \n",
    "        # Search\n",
    "        handle = Entrez.esearch(\n",
    "            db=\"pubmed\",\n",
    "            term=query,\n",
    "            retmax=max_results,\n",
    "            sort=\"relevance\"\n",
    "        )\n",
    "        results = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        \n",
    "        pmids = results[\"IdList\"]\n",
    "        print(f\"[PubMed] Found {len(pmids)} papers\")\n",
    "        \n",
    "        if not pmids:\n",
    "            return []\n",
    "        \n",
    "        # Fetch details\n",
    "        handle = Entrez.efetch(\n",
    "            db=\"pubmed\",\n",
    "            id=\",\".join(pmids),\n",
    "            rettype=\"xml\"\n",
    "        )\n",
    "        records = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        \n",
    "        papers = []\n",
    "        for article in records[\"PubmedArticle\"]:\n",
    "            try:\n",
    "                paper = self._parse_article(article)\n",
    "                if paper:\n",
    "                    papers.append(paper)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        return papers\n",
    "    \n",
    "    def _parse_article(self, article) -> Optional[Paper]:\n",
    "        \"\"\"Parse PubMed article into Paper object.\"\"\"\n",
    "        medline = article[\"MedlineCitation\"]\n",
    "        article_data = medline[\"Article\"]\n",
    "        \n",
    "        # Authors\n",
    "        authors = []\n",
    "        if \"AuthorList\" in article_data:\n",
    "            for author in article_data[\"AuthorList\"]:\n",
    "                if \"LastName\" in author:\n",
    "                    name = author[\"LastName\"]\n",
    "                    if \"ForeName\" in author:\n",
    "                        name = f\"{author['ForeName']} {name}\"\n",
    "                    elif \"Initials\" in author:\n",
    "                        name = f\"{author['Initials']} {name}\"\n",
    "                    authors.append(name)\n",
    "        \n",
    "        # Abstract\n",
    "        abstract = \"\"\n",
    "        if \"Abstract\" in article_data:\n",
    "            abstract_texts = article_data[\"Abstract\"][\"AbstractText\"]\n",
    "            if isinstance(abstract_texts, list):\n",
    "                abstract = \" \".join(str(t) for t in abstract_texts)\n",
    "            else:\n",
    "                abstract = str(abstract_texts)\n",
    "        \n",
    "        # Year\n",
    "        year = None\n",
    "        if \"DateCompleted\" in medline:\n",
    "            year = int(medline[\"DateCompleted\"][\"Year\"])\n",
    "        elif \"DateRevised\" in medline:\n",
    "            year = int(medline[\"DateRevised\"][\"Year\"])\n",
    "        \n",
    "        # DOI\n",
    "        doi = None\n",
    "        if \"ELocationID\" in article_data:\n",
    "            for eid in article_data[\"ELocationID\"]:\n",
    "                if str(eid.attributes.get(\"EIdType\", \"\")) == \"doi\":\n",
    "                    doi = str(eid)\n",
    "        \n",
    "        return Paper(\n",
    "            title=str(article_data[\"ArticleTitle\"]),\n",
    "            authors=authors,\n",
    "            year=year,\n",
    "            abstract=abstract,\n",
    "            source=\"PubMed\",\n",
    "            pmid=str(medline[\"PMID\"]),\n",
    "            doi=doi,\n",
    "            journal=str(article_data[\"Journal\"][\"Title\"]) if \"Journal\" in article_data else None,\n",
    "            url=f\"https://pubmed.ncbi.nlm.nih.gov/{medline['PMID']}/\"\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"PubMed engine loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Scholar Search Engine\n",
    "\n",
    "Two options:\n",
    "1. **SerpAPI** (paid, reliable) - requires API key\n",
    "2. **scholarly** (free, rate-limited) - may get blocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleScholarEngine(SearchEngine):\n",
    "    \"\"\"Search Google Scholar via SerpAPI or scholarly library.\"\"\"\n",
    "    \n",
    "    name = \"Google Scholar\"\n",
    "    \n",
    "    def __init__(self, credentials: SearchCredentials, use_serpapi: bool = True):\n",
    "        super().__init__(credentials)\n",
    "        self.use_serpapi = use_serpapi and bool(credentials.serpapi_key)\n",
    "    \n",
    "    def is_available(self) -> bool:\n",
    "        if self.use_serpapi:\n",
    "            try:\n",
    "                from serpapi import GoogleSearch\n",
    "                return bool(self.credentials.serpapi_key)\n",
    "            except ImportError:\n",
    "                pass\n",
    "        \n",
    "        # Fall back to scholarly\n",
    "        try:\n",
    "            import scholarly\n",
    "            self.use_serpapi = False\n",
    "            return True\n",
    "        except ImportError:\n",
    "            return False\n",
    "    \n",
    "    def search(self, query: str, max_results: int = 100) -> List[Paper]:\n",
    "        if self.use_serpapi:\n",
    "            return self._search_serpapi(query, max_results)\n",
    "        else:\n",
    "            return self._search_scholarly(query, max_results)\n",
    "    \n",
    "    def _search_serpapi(self, query: str, max_results: int) -> List[Paper]:\n",
    "        \"\"\"Search using SerpAPI (paid, reliable).\"\"\"\n",
    "        from serpapi import GoogleSearch\n",
    "        \n",
    "        papers = []\n",
    "        start = 0\n",
    "        \n",
    "        while len(papers) < max_results:\n",
    "            params = {\n",
    "                \"engine\": \"google_scholar\",\n",
    "                \"q\": query,\n",
    "                \"api_key\": self.credentials.serpapi_key,\n",
    "                \"start\": start,\n",
    "                \"num\": min(20, max_results - len(papers))\n",
    "            }\n",
    "            \n",
    "            search = GoogleSearch(params)\n",
    "            results = search.get_dict()\n",
    "            \n",
    "            if \"organic_results\" not in results:\n",
    "                break\n",
    "            \n",
    "            for result in results[\"organic_results\"]:\n",
    "                paper = self._parse_serpapi_result(result)\n",
    "                if paper:\n",
    "                    papers.append(paper)\n",
    "            \n",
    "            start += 20\n",
    "            if len(results[\"organic_results\"]) < 20:\n",
    "                break\n",
    "        \n",
    "        print(f\"[Google Scholar/SerpAPI] Found {len(papers)} papers\")\n",
    "        return papers[:max_results]\n",
    "    \n",
    "    def _parse_serpapi_result(self, result: dict) -> Optional[Paper]:\n",
    "        \"\"\"Parse SerpAPI result into Paper.\"\"\"\n",
    "        # Extract authors from publication_info\n",
    "        authors = []\n",
    "        pub_info = result.get(\"publication_info\", {})\n",
    "        if \"authors\" in pub_info:\n",
    "            authors = [a.get(\"name\", \"\") for a in pub_info[\"authors\"]]\n",
    "        elif \"summary\" in pub_info:\n",
    "            # Try to parse from summary string\n",
    "            parts = pub_info[\"summary\"].split(\" - \")\n",
    "            if parts:\n",
    "                authors = [a.strip() for a in parts[0].split(\",\")]\n",
    "        \n",
    "        # Extract year\n",
    "        year = None\n",
    "        summary = pub_info.get(\"summary\", \"\")\n",
    "        import re\n",
    "        year_match = re.search(r'\\b(19|20)\\d{2}\\b', summary)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "        \n",
    "        return Paper(\n",
    "            title=result.get(\"title\", \"\"),\n",
    "            authors=authors,\n",
    "            year=year,\n",
    "            abstract=result.get(\"snippet\", \"\"),\n",
    "            source=\"Google Scholar\",\n",
    "            url=result.get(\"link\"),\n",
    "            citation_count=result.get(\"inline_links\", {}).get(\"cited_by\", {}).get(\"total\")\n",
    "        )\n",
    "    \n",
    "    def _search_scholarly(self, query: str, max_results: int) -> List[Paper]:\n",
    "        \"\"\"Search using scholarly library (free, rate-limited).\"\"\"\n",
    "        from scholarly import scholarly\n",
    "        \n",
    "        papers = []\n",
    "        search_query = scholarly.search_pubs(query)\n",
    "        \n",
    "        for i, result in enumerate(search_query):\n",
    "            if i >= max_results:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                paper = self._parse_scholarly_result(result)\n",
    "                if paper:\n",
    "                    papers.append(paper)\n",
    "                \n",
    "                # Rate limiting to avoid blocks\n",
    "                if i > 0 and i % 10 == 0:\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not parse result {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"[Google Scholar/scholarly] Found {len(papers)} papers\")\n",
    "        return papers\n",
    "    \n",
    "    def _parse_scholarly_result(self, result) -> Optional[Paper]:\n",
    "        \"\"\"Parse scholarly result into Paper.\"\"\"\n",
    "        bib = result.get(\"bib\", {})\n",
    "        \n",
    "        # Authors\n",
    "        authors = bib.get(\"author\", [])\n",
    "        if isinstance(authors, str):\n",
    "            authors = [a.strip() for a in authors.split(\" and \")]\n",
    "        \n",
    "        # Year\n",
    "        year = None\n",
    "        if \"pub_year\" in bib:\n",
    "            try:\n",
    "                year = int(bib[\"pub_year\"])\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        return Paper(\n",
    "            title=bib.get(\"title\", \"\"),\n",
    "            authors=authors,\n",
    "            year=year,\n",
    "            abstract=bib.get(\"abstract\", \"\"),\n",
    "            source=\"Google Scholar\",\n",
    "            url=result.get(\"pub_url\") or result.get(\"eprint_url\"),\n",
    "            citation_count=result.get(\"num_citations\")\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Google Scholar engine loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Scholar Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticScholarEngine(SearchEngine):\n",
    "    \"\"\"Search Semantic Scholar API (free, high quality).\"\"\"\n",
    "    \n",
    "    name = \"Semantic Scholar\"\n",
    "    BASE_URL = \"https://api.semanticscholar.org/graph/v1\"\n",
    "    \n",
    "    def is_available(self) -> bool:\n",
    "        try:\n",
    "            import requests\n",
    "            return True\n",
    "        except ImportError:\n",
    "            return False\n",
    "    \n",
    "    def search(self, query: str, max_results: int = 100) -> List[Paper]:\n",
    "        import requests\n",
    "        \n",
    "        headers = {}\n",
    "        if self.credentials.semantic_scholar_key:\n",
    "            headers[\"x-api-key\"] = self.credentials.semantic_scholar_key\n",
    "        \n",
    "        papers = []\n",
    "        offset = 0\n",
    "        \n",
    "        while len(papers) < max_results:\n",
    "            params = {\n",
    "                \"query\": query,\n",
    "                \"offset\": offset,\n",
    "                \"limit\": min(100, max_results - len(papers)),\n",
    "                \"fields\": \"paperId,title,abstract,authors,year,citationCount,externalIds,url,venue\"\n",
    "            }\n",
    "            \n",
    "            response = requests.get(\n",
    "                f\"{self.BASE_URL}/paper/search\",\n",
    "                params=params,\n",
    "                headers=headers\n",
    "            )\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"  Warning: Semantic Scholar API error {response.status_code}\")\n",
    "                break\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            if \"data\" not in data or not data[\"data\"]:\n",
    "                break\n",
    "            \n",
    "            for item in data[\"data\"]:\n",
    "                paper = self._parse_result(item)\n",
    "                if paper:\n",
    "                    papers.append(paper)\n",
    "            \n",
    "            offset += len(data[\"data\"])\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            if len(data[\"data\"]) < 100:\n",
    "                break\n",
    "        \n",
    "        print(f\"[Semantic Scholar] Found {len(papers)} papers\")\n",
    "        return papers[:max_results]\n",
    "    \n",
    "    def _parse_result(self, item: dict) -> Optional[Paper]:\n",
    "        \"\"\"Parse Semantic Scholar result into Paper.\"\"\"\n",
    "        # Authors\n",
    "        authors = [a.get(\"name\", \"\") for a in item.get(\"authors\", [])]\n",
    "        \n",
    "        # External IDs\n",
    "        ext_ids = item.get(\"externalIds\", {}) or {}\n",
    "        \n",
    "        return Paper(\n",
    "            title=item.get(\"title\", \"\"),\n",
    "            authors=authors,\n",
    "            year=item.get(\"year\"),\n",
    "            abstract=item.get(\"abstract\", \"\") or \"\",\n",
    "            source=\"Semantic Scholar\",\n",
    "            semantic_scholar_id=item.get(\"paperId\"),\n",
    "            doi=ext_ids.get(\"DOI\"),\n",
    "            pmid=ext_ids.get(\"PubMed\"),\n",
    "            arxiv_id=ext_ids.get(\"ArXiv\"),\n",
    "            url=item.get(\"url\"),\n",
    "            journal=item.get(\"venue\"),\n",
    "            citation_count=item.get(\"citationCount\")\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Semantic Scholar engine loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scopus Search Engine (Institutional Access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScopusEngine(SearchEngine):\n",
    "    \"\"\"Search Scopus via Elsevier API (requires institutional key).\"\"\"\n",
    "    \n",
    "    name = \"Scopus\"\n",
    "    BASE_URL = \"https://api.elsevier.com/content/search/scopus\"\n",
    "    \n",
    "    def is_available(self) -> bool:\n",
    "        try:\n",
    "            import requests\n",
    "            return bool(self.credentials.scopus_api_key)\n",
    "        except ImportError:\n",
    "            return False\n",
    "    \n",
    "    def search(self, query: str, max_results: int = 100) -> List[Paper]:\n",
    "        import requests\n",
    "        \n",
    "        headers = {\n",
    "            \"X-ELS-APIKey\": self.credentials.scopus_api_key,\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        if self.credentials.scopus_inst_token:\n",
    "            headers[\"X-ELS-Insttoken\"] = self.credentials.scopus_inst_token\n",
    "        \n",
    "        papers = []\n",
    "        start = 0\n",
    "        \n",
    "        while len(papers) < max_results:\n",
    "            params = {\n",
    "                \"query\": query,\n",
    "                \"start\": start,\n",
    "                \"count\": min(25, max_results - len(papers)),\n",
    "                \"view\": \"COMPLETE\"\n",
    "            }\n",
    "            \n",
    "            response = requests.get(self.BASE_URL, params=params, headers=headers)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"  Warning: Scopus API error {response.status_code}\")\n",
    "                break\n",
    "            \n",
    "            data = response.json()\n",
    "            results = data.get(\"search-results\", {}).get(\"entry\", [])\n",
    "            \n",
    "            if not results:\n",
    "                break\n",
    "            \n",
    "            for item in results:\n",
    "                paper = self._parse_result(item)\n",
    "                if paper:\n",
    "                    papers.append(paper)\n",
    "            \n",
    "            start += len(results)\n",
    "            \n",
    "            if len(results) < 25:\n",
    "                break\n",
    "        \n",
    "        print(f\"[Scopus] Found {len(papers)} papers\")\n",
    "        return papers[:max_results]\n",
    "    \n",
    "    def _parse_result(self, item: dict) -> Optional[Paper]:\n",
    "        \"\"\"Parse Scopus result into Paper.\"\"\"\n",
    "        # Authors\n",
    "        authors = []\n",
    "        if \"author\" in item:\n",
    "            for author in item[\"author\"]:\n",
    "                name = author.get(\"authname\", \"\")\n",
    "                if name:\n",
    "                    authors.append(name)\n",
    "        \n",
    "        # Year from cover date\n",
    "        year = None\n",
    "        cover_date = item.get(\"prism:coverDate\", \"\")\n",
    "        if cover_date:\n",
    "            try:\n",
    "                year = int(cover_date[:4])\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        return Paper(\n",
    "            title=item.get(\"dc:title\", \"\"),\n",
    "            authors=authors,\n",
    "            year=year,\n",
    "            abstract=item.get(\"dc:description\", \"\") or \"\",\n",
    "            source=\"Scopus\",\n",
    "            doi=item.get(\"prism:doi\"),\n",
    "            url=item.get(\"link\", [{}])[0].get(\"@href\") if item.get(\"link\") else None,\n",
    "            journal=item.get(\"prism:publicationName\"),\n",
    "            citation_count=int(item.get(\"citedby-count\", 0)) if item.get(\"citedby-count\") else None\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Scopus engine loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Multi-Engine Search Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchManager:\n",
    "    \"\"\"Manage searches across multiple engines.\"\"\"\n",
    "    \n",
    "    def __init__(self, credentials: SearchCredentials):\n",
    "        self.credentials = credentials\n",
    "        self.engines: Dict[str, SearchEngine] = {}\n",
    "        self._register_engines()\n",
    "    \n",
    "    def _register_engines(self):\n",
    "        \"\"\"Register all available search engines.\"\"\"\n",
    "        engine_classes = [\n",
    "            PubMedEngine,\n",
    "            GoogleScholarEngine,\n",
    "            SemanticScholarEngine,\n",
    "            ScopusEngine\n",
    "        ]\n",
    "        \n",
    "        for engine_class in engine_classes:\n",
    "            try:\n",
    "                engine = engine_class(self.credentials)\n",
    "                if engine.is_available():\n",
    "                    self.engines[engine.name] = engine\n",
    "            except Exception as e:\n",
    "                print(f\"  Could not initialize {engine_class.name}: {e}\")\n",
    "    \n",
    "    def list_engines(self) -> List[str]:\n",
    "        \"\"\"List available search engines.\"\"\"\n",
    "        return list(self.engines.keys())\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        engines: Optional[List[str]] = None,\n",
    "        max_results_per_engine: int = 50,\n",
    "        deduplicate: bool = True\n",
    "    ) -> List[Paper]:\n",
    "        \"\"\"\n",
    "        Search across multiple engines.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            engines: List of engine names to use (None = all available)\n",
    "            max_results_per_engine: Max results from each engine\n",
    "            deduplicate: Remove duplicate papers (by DOI/title)\n",
    "        \n",
    "        Returns:\n",
    "            Combined list of papers\n",
    "        \"\"\"\n",
    "        if engines is None:\n",
    "            engines = list(self.engines.keys())\n",
    "        \n",
    "        all_papers = []\n",
    "        \n",
    "        for engine_name in engines:\n",
    "            if engine_name not in self.engines:\n",
    "                print(f\"  Warning: {engine_name} not available, skipping\")\n",
    "                continue\n",
    "            \n",
    "            engine = self.engines[engine_name]\n",
    "            try:\n",
    "                papers = engine.search(query, max_results=max_results_per_engine)\n",
    "                all_papers.extend(papers)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error searching {engine_name}: {e}\")\n",
    "        \n",
    "        if deduplicate:\n",
    "            all_papers = self._deduplicate(all_papers)\n",
    "        \n",
    "        print(f\"\\nTotal unique papers: {len(all_papers)}\")\n",
    "        return all_papers\n",
    "    \n",
    "    def _deduplicate(self, papers: List[Paper]) -> List[Paper]:\n",
    "        \"\"\"Remove duplicate papers based on DOI or title similarity.\"\"\"\n",
    "        seen_dois = set()\n",
    "        seen_titles = set()\n",
    "        unique = []\n",
    "        \n",
    "        for paper in papers:\n",
    "            # Check DOI\n",
    "            if paper.doi:\n",
    "                if paper.doi.lower() in seen_dois:\n",
    "                    continue\n",
    "                seen_dois.add(paper.doi.lower())\n",
    "            \n",
    "            # Check title (normalized)\n",
    "            normalized_title = paper.title.lower().strip()[:100]\n",
    "            if normalized_title in seen_titles:\n",
    "                continue\n",
    "            seen_titles.add(normalized_title)\n",
    "            \n",
    "            unique.append(paper)\n",
    "        \n",
    "        return unique\n",
    "\n",
    "\n",
    "# Initialize search manager\n",
    "search_manager = SearchManager(credentials)\n",
    "\n",
    "print(\"\\nAvailable search engines:\")\n",
    "for engine in search_manager.list_engines():\n",
    "    print(f\"  ✓ {engine}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build search query\n",
    "SEARCH_TERMS = [\n",
    "    '(\"T-maze\" OR \"T maze\" OR \"spatial decision\" OR \"spatial navigation\")',\n",
    "    '(fMRI OR \"functional MRI\" OR \"functional magnetic resonance\")',\n",
    "    '(activation OR BOLD OR \"brain activity\")'\n",
    "]\n",
    "\n",
    "SEARCH_QUERY = \" AND \".join(SEARCH_TERMS)\n",
    "print(\"Search Query:\")\n",
    "print(SEARCH_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multi-engine search\n",
    "# Option 1: Search all available engines\n",
    "papers = search_manager.search(\n",
    "    query=SEARCH_QUERY,\n",
    "    max_results_per_engine=30,\n",
    "    deduplicate=True\n",
    ")\n",
    "\n",
    "# Option 2: Search specific engines only\n",
    "# papers = search_manager.search(\n",
    "#     query=SEARCH_QUERY,\n",
    "#     engines=[\"PubMed\", \"Semantic Scholar\"],\n",
    "#     max_results_per_engine=50\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for viewing\n",
    "papers_df = pd.DataFrame([p.to_dict() for p in papers])\n",
    "\n",
    "print(f\"\\nResults by source:\")\n",
    "print(papers_df[\"source\"].value_counts())\n",
    "\n",
    "papers_df[[\"title\", \"year\", \"source\", \"citation_count\"]].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: AI-Assisted Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extraction.extractors.base_extractor import LLMProvider\n",
    "\n",
    "# Initialize LLM (requires ANTHROPIC_API_KEY environment variable)\n",
    "try:\n",
    "    llm = LLMProvider(provider=\"anthropic\")\n",
    "    llm_available = True\n",
    "    print(\"LLM initialized for screening\")\n",
    "except Exception as e:\n",
    "    print(f\"LLM not available: {e}\")\n",
    "    print(\"Set ANTHROPIC_API_KEY environment variable to enable AI screening\")\n",
    "    llm_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCREENING_PROMPT = \"\"\"You are screening abstracts for a meta-analysis.\n",
    "\n",
    "Research Question: {question}\n",
    "\n",
    "INCLUSION CRITERIA:\n",
    "{inclusion}\n",
    "\n",
    "EXCLUSION CRITERIA:\n",
    "{exclusion}\n",
    "\n",
    "Paper to screen:\n",
    "Title: {title}\n",
    "Year: {year}\n",
    "Abstract: {abstract}\n",
    "\n",
    "Based on the abstract, determine:\n",
    "1. INCLUDE, EXCLUDE, or UNCERTAIN\n",
    "2. Which criteria are met/not met\n",
    "3. Brief reasoning\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "    \"decision\": \"INCLUDE\" | \"EXCLUDE\" | \"UNCERTAIN\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"reasoning\": \"brief explanation\",\n",
    "    \"criteria_met\": [\"list of met inclusion criteria\"],\n",
    "    \"exclusion_reasons\": [\"list of exclusion reasons, if any\"]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def screen_paper(paper: Paper) -> dict:\n",
    "    \"\"\"Screen a single paper using AI.\"\"\"\n",
    "    prompt = SCREENING_PROMPT.format(\n",
    "        question=RESEARCH_QUESTION,\n",
    "        inclusion=\"\\n\".join(f\"- {c}\" for c in INCLUSION_CRITERIA),\n",
    "        exclusion=\"\\n\".join(f\"- {c}\" for c in EXCLUSION_CRITERIA),\n",
    "        title=paper.title,\n",
    "        year=paper.year,\n",
    "        abstract=paper.abstract\n",
    "    )\n",
    "    \n",
    "    result = llm.extract(paper.abstract, prompt)\n",
    "    result[\"title\"] = paper.title\n",
    "    result[\"source\"] = paper.source\n",
    "    result[\"doi\"] = paper.doi\n",
    "    result[\"pmid\"] = paper.pmid\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Screen papers (costs money per API call!)\n",
    "# Limit to first N papers for demo\n",
    "N_TO_SCREEN = 5\n",
    "\n",
    "screening_results = []\n",
    "\n",
    "if llm_available and papers:\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Filter to papers with abstracts\n",
    "    papers_with_abstracts = [p for p in papers if p.abstract]\n",
    "    \n",
    "    for paper in tqdm(papers_with_abstracts[:N_TO_SCREEN], desc=\"Screening\"):\n",
    "        try:\n",
    "            result = screen_paper(paper)\n",
    "            screening_results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error screening '{paper.title[:50]}...': {e}\")\n",
    "    \n",
    "    screening_df = pd.DataFrame(screening_results)\n",
    "    print(f\"\\nScreened {len(screening_results)} papers\")\n",
    "else:\n",
    "    print(\"Skipping AI screening (LLM not available or no papers)\")\n",
    "    screening_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View screening results\n",
    "if len(screening_df) > 0:\n",
    "    print(\"Screening Summary:\")\n",
    "    print(screening_df[\"decision\"].value_counts())\n",
    "    print()\n",
    "    screening_df[[\"title\", \"decision\", \"confidence\", \"reasoning\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_dir = Path(\"../data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save all search results\n",
    "papers_df.to_csv(output_dir / \"search_results.csv\", index=False)\n",
    "print(f\"Saved {len(papers_df)} papers to {output_dir / 'search_results.csv'}\")\n",
    "\n",
    "# Save screening results\n",
    "if len(screening_df) > 0:\n",
    "    screening_df.to_csv(output_dir / \"screening_results.csv\", index=False)\n",
    "    print(f\"Saved screening results to {output_dir / 'screening_results.csv'}\")\n",
    "\n",
    "# Save search metadata for PRISMA\n",
    "search_metadata = {\n",
    "    \"search_date\": datetime.now().isoformat(),\n",
    "    \"query\": SEARCH_QUERY,\n",
    "    \"research_question\": RESEARCH_QUESTION,\n",
    "    \"pico\": PICO,\n",
    "    \"inclusion_criteria\": INCLUSION_CRITERIA,\n",
    "    \"exclusion_criteria\": EXCLUSION_CRITERIA,\n",
    "    \"engines_used\": search_manager.list_engines(),\n",
    "    \"total_results\": len(papers),\n",
    "    \"results_by_source\": papers_df[\"source\"].value_counts().to_dict() if len(papers_df) > 0 else {}\n",
    "}\n",
    "\n",
    "with open(output_dir / \"search_metadata.json\", \"w\") as f:\n",
    "    json.dump(search_metadata, f, indent=2)\n",
    "print(f\"Saved search metadata to {output_dir / 'search_metadata.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRISMA Flow Diagram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate PRISMA numbers\n",
    "prisma = {\n",
    "    \"identification\": {\n",
    "        \"records_identified\": len(papers),\n",
    "        \"by_source\": papers_df[\"source\"].value_counts().to_dict() if len(papers_df) > 0 else {}\n",
    "    },\n",
    "    \"screening\": {\n",
    "        \"records_screened\": len(screening_df) if len(screening_df) > 0 else 0,\n",
    "        \"records_excluded\": len(screening_df[screening_df[\"decision\"] == \"EXCLUDE\"]) if len(screening_df) > 0 else 0\n",
    "    },\n",
    "    \"eligibility\": {\n",
    "        \"full_text_assessed\": len(screening_df[screening_df[\"decision\"] == \"INCLUDE\"]) if len(screening_df) > 0 else 0,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"PRISMA Flow Data:\")\n",
    "print(json.dumps(prisma, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Review UNCERTAIN papers** manually\n",
    "2. **Get full-text PDFs** for INCLUDED papers\n",
    "3. **Proceed to notebook 02** for data extraction\n",
    "\n",
    "### Installing Additional Dependencies\n",
    "\n",
    "```bash\n",
    "# For PubMed\n",
    "pip install biopython\n",
    "\n",
    "# For Google Scholar (free option)\n",
    "pip install scholarly\n",
    "\n",
    "# For Google Scholar (paid, reliable)\n",
    "pip install google-search-results  # SerpAPI\n",
    "\n",
    "# For Semantic Scholar / Scopus\n",
    "pip install requests\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
