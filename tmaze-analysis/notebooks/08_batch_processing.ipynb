{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-Maze Batch Processing & HPC Integration\n",
    "\n",
    "This notebook demonstrates automated pipelines:\n",
    "- Batch processing with checkpointing\n",
    "- Slurm job submission for HPC\n",
    "- BIDS format handling\n",
    "- Report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from pipelines.batch import (\n",
    "    BatchProcessor,\n",
    "    BatchConfig,\n",
    "    parallel_map,\n",
    "    checkpoint_resume,\n",
    "    create_processing_report\n",
    ")\n",
    "from pipelines.hpc import (\n",
    "    SlurmSubmitter,\n",
    "    generate_slurm_template\n",
    ")\n",
    "from pipelines.bids import (\n",
    "    BIDSDataset,\n",
    "    validate_bids,\n",
    "    bids_to_tmaze,\n",
    "    tmaze_to_bids\n",
    ")\n",
    "from pipelines.reporting import (\n",
    "    generate_report,\n",
    "    statistics_table,\n",
    "    export_for_publication\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample processing function\n",
    "def process_subject(subject_id, data_dir=None, param1=1.0):\n",
    "    \"\"\"\n",
    "    Example processing function for a single subject.\n",
    "    In practice, this would load data and run analysis.\n",
    "    \"\"\"\n",
    "    # Simulate processing time\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    # Simulate results\n",
    "    np.random.seed(hash(subject_id) % 2**32)\n",
    "    \n",
    "    return {\n",
    "        'subject_id': subject_id,\n",
    "        'accuracy': 0.5 + np.random.rand() * 0.3,\n",
    "        'n_trials': np.random.randint(80, 120),\n",
    "        'parameter': param1\n",
    "    }\n",
    "\n",
    "# Create list of subjects\n",
    "subject_ids = [f'sub-{i:02d}' for i in range(1, 11)]\n",
    "print(f\"Processing {len(subject_ids)} subjects: {subject_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure batch processing\n",
    "config = BatchConfig(\n",
    "    n_jobs=4,  # Number of parallel jobs\n",
    "    verbose=10,\n",
    "    checkpoint_dir=Path('./checkpoints'),\n",
    "    checkpoint_freq=5,\n",
    "    continue_on_error=True,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "# Create processor\n",
    "processor = BatchProcessor(process_subject, config)\n",
    "\n",
    "# Run batch processing\n",
    "print(\"\\nStarting batch processing...\")\n",
    "batch_result = processor.run(\n",
    "    subject_ids,\n",
    "    data_dir='/path/to/data',\n",
    "    param1=2.0\n",
    ")\n",
    "\n",
    "print(f\"\\n{batch_result.summary()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate processing report\n",
    "report = create_processing_report(batch_result)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results from successful subjects\n",
    "successful = batch_result.get_successful()\n",
    "accuracies = [r.result['accuracy'] for r in successful]\n",
    "\n",
    "print(f\"\\nSuccessful subjects: {len(successful)}\")\n",
    "print(f\"Mean accuracy: {np.mean(accuracies):.3f} Â± {np.std(accuracies):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parallel Processing with parallel_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple parallel processing\n",
    "def compute_roi_accuracy(roi_idx, X=None, y=None):\n",
    "    \"\"\"Compute accuracy for a single ROI.\"\"\"\n",
    "    time.sleep(0.05)  # Simulate computation\n",
    "    return {'roi': roi_idx, 'accuracy': 0.5 + np.random.rand() * 0.2}\n",
    "\n",
    "# Parallel map across ROIs\n",
    "roi_indices = list(range(50))\n",
    "\n",
    "print(\"Running parallel_map on 50 ROIs...\")\n",
    "start = time.time()\n",
    "\n",
    "results = parallel_map(\n",
    "    compute_roi_accuracy,\n",
    "    roi_indices,\n",
    "    n_jobs=4,\n",
    "    verbose=5\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\nCompleted in {elapsed:.2f}s\")\n",
    "print(f\"Results: {len(results)} ROIs processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Checkpoint and Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing with checkpoint/resume\n",
    "def long_running_analysis(subject_id):\n",
    "    time.sleep(0.2)  # Simulate long computation\n",
    "    return {'subject': subject_id, 'result': np.random.rand()}\n",
    "\n",
    "checkpoint_file = Path('./analysis_checkpoint.json')\n",
    "\n",
    "# First run - process some subjects\n",
    "print(\"First run (will checkpoint after each subject)...\")\n",
    "partial_subjects = subject_ids[:5]\n",
    "\n",
    "results = checkpoint_resume(\n",
    "    long_running_analysis,\n",
    "    partial_subjects,\n",
    "    checkpoint_file\n",
    ")\n",
    "\n",
    "print(f\"Processed: {len(results)} subjects\")\n",
    "\n",
    "# Second run - resume with more subjects\n",
    "print(\"\\nSecond run (resuming with all subjects)...\")\n",
    "all_results = checkpoint_resume(\n",
    "    long_running_analysis,\n",
    "    subject_ids,\n",
    "    checkpoint_file\n",
    ")\n",
    "\n",
    "print(f\"Total processed: {len(all_results)} subjects\")\n",
    "\n",
    "# Cleanup\n",
    "if checkpoint_file.exists():\n",
    "    checkpoint_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HPC Job Submission (Slurm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Slurm submitter\n",
    "submitter = SlurmSubmitter(\n",
    "    partition='main',\n",
    "    time='04:00:00',\n",
    "    mem='16G',\n",
    "    cpus_per_task=4,\n",
    "    account='jss388',  # Your account\n",
    "    output_dir=Path('./slurm_jobs'),\n",
    "    conda_env='tmaze'\n",
    ")\n",
    "\n",
    "print(\"Slurm configuration:\")\n",
    "print(f\"  Partition: {submitter.partition}\")\n",
    "print(f\"  Time limit: {submitter.time}\")\n",
    "print(f\"  Memory: {submitter.mem}\")\n",
    "print(f\"  CPUs: {submitter.cpus_per_task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a template script (dry run)\n",
    "template_path = Path('./slurm_jobs/analysis_template.sh')\n",
    "\n",
    "generate_slurm_template(\n",
    "    template_path,\n",
    "    python_script='run_analysis.py',\n",
    "    job_name='tmaze_analysis',\n",
    "    partition='main',\n",
    "    time='04:00:00',\n",
    "    mem='16G',\n",
    "    cpus=4,\n",
    "    conda_env='tmaze'\n",
    ")\n",
    "\n",
    "# Display template\n",
    "print(\"\\nGenerated Slurm template:\")\n",
    "print(\"-\" * 50)\n",
    "with open(template_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit job array (dry run)\n",
    "print(\"\\nSubmitting job array (DRY RUN)...\")\n",
    "\n",
    "jobs = submitter.submit_array(\n",
    "    python_script='scripts/run_subject_analysis.py',\n",
    "    subject_ids=subject_ids,\n",
    "    job_name='tmaze_batch',\n",
    "    array_batch_size=5,\n",
    "    dry_run=True,  # Don't actually submit\n",
    "    output_dir='./results'\n",
    ")\n",
    "\n",
    "print(f\"\\nWould submit {len(jobs)} job(s):\")\n",
    "for job in jobs:\n",
    "    print(f\"  Job: {job.name}\")\n",
    "    print(f\"  Script: {job.script_path}\")\n",
    "    print(f\"  Subjects: {len(job.metadata.get('subjects', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BIDS Format Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock BIDS structure for demonstration\n",
    "bids_root = Path('./mock_bids')\n",
    "bids_root.mkdir(exist_ok=True)\n",
    "\n",
    "# Create required files\n",
    "desc = {\n",
    "    'Name': 'T-Maze EEG-fMRI Dataset',\n",
    "    'BIDSVersion': '1.8.0',\n",
    "    'Authors': ['Test Author']\n",
    "}\n",
    "with open(bids_root / 'dataset_description.json', 'w') as f:\n",
    "    json.dump(desc, f)\n",
    "\n",
    "# Create participants.tsv\n",
    "with open(bids_root / 'participants.tsv', 'w') as f:\n",
    "    f.write('participant_id\\tage\\tsex\\n')\n",
    "    for i in range(1, 6):\n",
    "        f.write(f'sub-{i:02d}\\t{20+i}\\tM\\n')\n",
    "\n",
    "# Create subject directories\n",
    "for i in range(1, 6):\n",
    "    subj_dir = bids_root / f'sub-{i:02d}' / 'eeg'\n",
    "    subj_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (subj_dir / f'sub-{i:02d}_task-tmaze_eeg.set').touch()\n",
    "\n",
    "print(f\"Created mock BIDS dataset at: {bids_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate BIDS structure\n",
    "validation = validate_bids(bids_root)\n",
    "\n",
    "print(\"BIDS Validation Results:\")\n",
    "print(f\"  Valid: {validation['valid']}\")\n",
    "print(f\"  Subjects: {validation['n_subjects']}\")\n",
    "if validation['issues']:\n",
    "    print(f\"  Issues: {validation['issues']}\")\n",
    "if validation['warnings']:\n",
    "    print(f\"  Warnings: {validation['warnings']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BIDS dataset\n",
    "dataset = BIDSDataset(bids_root, validate=False)\n",
    "\n",
    "subjects = dataset.get_subjects()\n",
    "print(f\"\\nSubjects in dataset: {subjects}\")\n",
    "\n",
    "# Get files for a subject\n",
    "subj = subjects[0]\n",
    "eeg_files = dataset.get_eeg_files(subj, task='tmaze')\n",
    "print(f\"\\nEEG files for {subj}: {eeg_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to BIDS derivatives format\n",
    "analysis_results = {\n",
    "    'sub-01': {'accuracy': 0.72, 'auc': 0.78},\n",
    "    'sub-02': {'accuracy': 0.68, 'auc': 0.74},\n",
    "    'sub-03': {'accuracy': 0.75, 'auc': 0.81}\n",
    "}\n",
    "\n",
    "deriv_dir = tmaze_to_bids(\n",
    "    analysis_results,\n",
    "    output_dir=bids_root,\n",
    "    analysis_name='tmaze-classification',\n",
    "    description='T-maze reward vs no-reward classification'\n",
    ")\n",
    "\n",
    "print(f\"\\nDerivatives exported to: {deriv_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results for report\n",
    "report_results = {\n",
    "    'n_subjects': 20,\n",
    "    'classification': {\n",
    "        'accuracy': 0.72,\n",
    "        'accuracy_std': 0.08,\n",
    "        'auc': 0.78\n",
    "    },\n",
    "    'group_stats': {\n",
    "        'Overall': {\n",
    "            'statistic': 4.52,\n",
    "            'p_value': 0.0002,\n",
    "            'effect_size': 0.85\n",
    "        },\n",
    "        'Reward Effect': {\n",
    "            'statistic': 3.21,\n",
    "            'p_value': 0.003,\n",
    "            'effect_size': 0.62\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate HTML report\n",
    "report_path = generate_report(\n",
    "    report_results,\n",
    "    output_path=Path('./tmaze_report.html')\n",
    ")\n",
    "\n",
    "print(f\"Report generated: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate statistics table in different formats\n",
    "stats = report_results['group_stats']\n",
    "\n",
    "print(\"\\nMarkdown Table:\")\n",
    "print(statistics_table(stats, format='markdown'))\n",
    "\n",
    "print(\"\\n\\nLaTeX Table:\")\n",
    "print(statistics_table(stats, format='latex'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export for publication\n",
    "pub_dir = Path('./publication_outputs')\n",
    "exported = export_for_publication(\n",
    "    report_results,\n",
    "    pub_dir,\n",
    "    format='latex'\n",
    ")\n",
    "\n",
    "print(f\"\\nExported files:\")\n",
    "for name, path in exported.items():\n",
    "    print(f\"  {name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup mock files\n",
    "import shutil\n",
    "for p in ['./mock_bids', './checkpoints', './slurm_jobs', './publication_outputs']:\n",
    "    if Path(p).exists():\n",
    "        shutil.rmtree(p)\n",
    "        \n",
    "for f in ['./tmaze_report.html']:\n",
    "    if Path(f).exists():\n",
    "        Path(f).unlink()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE AUTOMATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Demonstrated capabilities:\n",
    "\n",
    "1. Batch Processing\n",
    "   - BatchProcessor for multi-subject analysis\n",
    "   - Parallel execution with joblib\n",
    "   - Automatic checkpointing and resume\n",
    "   - Error handling with retries\n",
    "\n",
    "2. HPC Integration (Slurm)\n",
    "   - SlurmSubmitter for job submission\n",
    "   - Job array support for batch subjects\n",
    "   - Template generation\n",
    "   - Job monitoring\n",
    "\n",
    "3. BIDS Format\n",
    "   - Dataset validation\n",
    "   - File querying (EEG, fMRI, events)\n",
    "   - Derivatives export\n",
    "\n",
    "4. Reporting\n",
    "   - HTML report generation\n",
    "   - Statistics tables (Markdown, LaTeX)\n",
    "   - Publication export\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
