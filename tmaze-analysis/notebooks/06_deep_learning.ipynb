{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-Maze Deep Learning Classification\n",
    "\n",
    "This notebook demonstrates deep learning approaches for T-maze decoding:\n",
    "- EEGNet for EEG classification\n",
    "- LSTM for temporal patterns\n",
    "- Multimodal fusion\n",
    "- Model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Check for PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed. Run: pip install torch\")\n",
    "\n",
    "from deeplearning import (\n",
    "    EEGNet,\n",
    "    LSTMDecoder,\n",
    "    EEGTransformer,\n",
    "    ROIMLP,\n",
    "    CrossAttentionFusion\n",
    ")\n",
    "from deeplearning.training import (\n",
    "    DeepTrainer,\n",
    "    EarlyStopping,\n",
    "    cross_validate_deep\n",
    ")\n",
    "from deeplearning.interpretation import (\n",
    "    GradCAM,\n",
    "    IntegratedGradients,\n",
    "    feature_importance_deep\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulate EEG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate EEG epochs\n",
    "np.random.seed(42)\n",
    "\n",
    "n_epochs = 500\n",
    "n_channels = 64\n",
    "n_times = 256  # 1s at 256 Hz\n",
    "\n",
    "# Generate synthetic EEG data\n",
    "# Class 0: lower power in 200-300ms window\n",
    "# Class 1: higher power (simulating reward positivity)\n",
    "\n",
    "X_eeg = np.random.randn(n_epochs, n_channels, n_times) * 10  # Baseline noise\n",
    "y_eeg = np.random.randint(0, 2, n_epochs)\n",
    "\n",
    "# Add class-specific patterns\n",
    "rewp_window = slice(50, 80)  # 200-310ms at 256 Hz\n",
    "fcz_channels = [30, 31, 32]  # Frontocentral\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    if y_eeg[i] == 1:\n",
    "        # Add positive deflection (reward response)\n",
    "        for ch in fcz_channels:\n",
    "            X_eeg[i, ch, rewp_window] += np.sin(np.linspace(0, np.pi, 30)) * 15\n",
    "\n",
    "# Split data\n",
    "split = int(0.8 * n_epochs)\n",
    "X_train, X_test = X_eeg[:split], X_eeg[split:]\n",
    "y_train, y_test = y_eeg[:split], y_eeg[split:]\n",
    "\n",
    "print(f\"Training data: {X_train.shape}\")\n",
    "print(f\"Test data: {X_test.shape}\")\n",
    "print(f\"Class balance: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EEGNet Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train EEGNet\n",
    "eegnet = EEGNet(\n",
    "    n_classes=2,\n",
    "    n_channels=n_channels,\n",
    "    n_times=n_times,\n",
    "    f1=8,\n",
    "    d=2,\n",
    "    dropout=0.5,\n",
    "    learning_rate=1e-3\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "eegnet.compile(input_shape=(n_channels, n_times))\n",
    "print(f\"EEGNet parameters: {eegnet.n_parameters():,}\")\n",
    "\n",
    "# Train with early stopping\n",
    "trainer = DeepTrainer(eegnet)\n",
    "early_stop = EarlyStopping(patience=15, mode='max')\n",
    "\n",
    "result = trainer.train(\n",
    "    X_train, y_train,\n",
    "    X_val=X_test, y_val=y_test,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    early_stopping=early_stop,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal accuracy: {result.accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(result.train_losses, label='Train')\n",
    "if result.val_losses:\n",
    "    axes[0].plot(result.val_losses, label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(result.train_accuracies, label='Train')\n",
    "if result.val_accuracies:\n",
    "    axes[1].plot(result.val_accuracies, label='Validation')\n",
    "axes[1].axhline(0.5, color='gray', linestyle='--', label='Chance')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM\n",
    "lstm = LSTMDecoder(\n",
    "    n_classes=2,\n",
    "    hidden_size=64,\n",
    "    n_layers=2,\n",
    "    bidirectional=True,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "lstm.compile(input_shape=(n_channels, n_times))\n",
    "print(f\"LSTM parameters: {lstm.n_parameters():,}\")\n",
    "\n",
    "trainer_lstm = DeepTrainer(lstm)\n",
    "result_lstm = trainer_lstm.train(\n",
    "    X_train, y_train,\n",
    "    X_val=X_test, y_val=y_test,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    early_stopping=EarlyStopping(patience=10),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nLSTM accuracy: {result_lstm.accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "models = {\n",
    "    'EEGNet': (EEGNet, {'f1': 8, 'd': 2}),\n",
    "    'LSTM': (LSTMDecoder, {'hidden_size': 64}),\n",
    "    'Transformer': (EEGTransformer, {'d_model': 64, 'n_heads': 4})\n",
    "}\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "for name, (model_class, kwargs) in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {name}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    try:\n",
    "        model = model_class(n_classes=2, **kwargs)\n",
    "        model.compile(input_shape=(n_channels, n_times))\n",
    "        \n",
    "        trainer = DeepTrainer(model)\n",
    "        result = trainer.train(\n",
    "            X_train, y_train,\n",
    "            X_val=X_test, y_val=y_test,\n",
    "            epochs=30,\n",
    "            batch_size=32,\n",
    "            early_stopping=EarlyStopping(patience=10),\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        comparison_results[name] = {\n",
    "            'accuracy': result.accuracy,\n",
    "            'n_params': model.n_parameters()\n",
    "        }\n",
    "        print(f\"{name}: accuracy={result.accuracy:.3f}, params={model.n_parameters():,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name} failed: {e}\")\n",
    "\n",
    "# Plot comparison\n",
    "if comparison_results:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    names = list(comparison_results.keys())\n",
    "    accuracies = [comparison_results[n]['accuracy'] for n in names]\n",
    "    \n",
    "    ax.bar(names, accuracies, color='steelblue', edgecolor='black')\n",
    "    ax.axhline(0.5, color='red', linestyle='--', label='Chance')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Model Comparison')\n",
    "    ax.set_ylim([0.4, 1.0])\n",
    "    \n",
    "    for i, acc in enumerate(accuracies):\n",
    "        ax.text(i, acc + 0.02, f'{acc:.1%}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance using integrated gradients\n",
    "try:\n",
    "    importances = feature_importance_deep(\n",
    "        eegnet.model,\n",
    "        X_test[:50],\n",
    "        y_test[:50],\n",
    "        method='gradient',\n",
    "        n_samples=50\n",
    "    )\n",
    "    \n",
    "    # Plot channel importance\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Temporal importance (averaged across channels)\n",
    "    temporal_imp = np.mean(importances, axis=0)\n",
    "    times = np.linspace(0, 1, n_times)\n",
    "    axes[0].plot(times, temporal_imp)\n",
    "    axes[0].axvspan(0.2, 0.35, alpha=0.2, color='yellow', label='REWP window')\n",
    "    axes[0].set_xlabel('Time (s)')\n",
    "    axes[0].set_ylabel('Importance')\n",
    "    axes[0].set_title('Temporal Feature Importance')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Channel importance (averaged across time)\n",
    "    channel_imp = np.mean(importances, axis=1)\n",
    "    axes[1].bar(range(len(channel_imp)), channel_imp)\n",
    "    axes[1].set_xlabel('Channel')\n",
    "    axes[1].set_ylabel('Importance')\n",
    "    axes[1].set_title('Channel Feature Importance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Interpretation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate EEGNet\n",
    "cv_result = cross_validate_deep(\n",
    "    EEGNet,\n",
    "    X_eeg, y_eeg,\n",
    "    n_folds=5,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    early_stopping=True,\n",
    "    verbose=False,\n",
    "    # Model kwargs\n",
    "    n_classes=2,\n",
    "    f1=8,\n",
    "    d=2\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print('='*50)\n",
    "print(f\"Mean accuracy: {cv_result.accuracy:.3f} Â± {cv_result.accuracy_std:.3f}\")\n",
    "print(f\"Fold accuracies: {cv_result.cv_accuracies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. fMRI Classification with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate fMRI ROI data\n",
    "np.random.seed(42)\n",
    "\n",
    "n_trials = 400\n",
    "n_rois = 426  # HCP atlas\n",
    "\n",
    "# Generate synthetic ROI betas\n",
    "X_fmri = np.random.randn(n_trials, n_rois)\n",
    "y_fmri = np.random.randint(0, 2, n_trials)\n",
    "\n",
    "# Add signal to reward-related ROIs\n",
    "reward_rois = [100, 101, 102, 200, 201]  # Simulated VS, vmPFC\n",
    "for i in range(n_trials):\n",
    "    if y_fmri[i] == 1:\n",
    "        X_fmri[i, reward_rois] += 0.5\n",
    "\n",
    "# Split\n",
    "split = int(0.8 * n_trials)\n",
    "X_train_fmri, X_test_fmri = X_fmri[:split], X_fmri[split:]\n",
    "y_train_fmri, y_test_fmri = y_fmri[:split], y_fmri[split:]\n",
    "\n",
    "# Train MLP\n",
    "mlp = ROIMLP(\n",
    "    n_classes=2,\n",
    "    hidden_sizes=[256, 128, 64],\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "mlp.compile(input_shape=(n_rois,))\n",
    "print(f\"MLP parameters: {mlp.n_parameters():,}\")\n",
    "\n",
    "trainer_mlp = DeepTrainer(mlp)\n",
    "result_mlp = trainer_mlp.train(\n",
    "    X_train_fmri, y_train_fmri,\n",
    "    X_val=X_test_fmri, y_val=y_test_fmri,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    early_stopping=EarlyStopping(patience=10),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nfMRI MLP accuracy: {result_mlp.accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEEP LEARNING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'Accuracy':<12} {'Parameters':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'EEGNet':<20} {result.accuracy:<12.3f} {eegnet.n_parameters():<15,}\")\n",
    "print(f\"{'LSTM':<20} {result_lstm.accuracy:<12.3f} {lstm.n_parameters():<15,}\")\n",
    "print(f\"{'fMRI MLP':<20} {result_mlp.accuracy:<12.3f} {mlp.n_parameters():<15,}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
