{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Statistical Significance Testing\n",
    "\n",
    "Statistical inference for classification results using permutation and t-tests.\n",
    "\n",
    "**Contents:**\n",
    "1. Single-subject permutation testing\n",
    "2. Group-level t-tests (one-sample vs. chance)\n",
    "3. Multiple comparison correction (FDR, Bonferroni)\n",
    "4. Cluster-based permutation for temporal decoding\n",
    "5. Bootstrap confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.ndimage import label as scipy_label\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score,\n",
    "    StratifiedKFold,\n",
    "    permutation_test_score\n",
    ")\n",
    "\n",
    "# For multiple comparison correction\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Single-Subject Permutation Testing\n",
    "\n",
    "Test if classification accuracy is significantly above chance by comparing to a null distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example classification data\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 120\n",
    "n_features = 50\n",
    "\n",
    "# Data with true signal\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = np.random.randint(0, 2, n_samples)\n",
    "\n",
    "# Add signal\n",
    "X[y == 1, :5] += 0.8\n",
    "\n",
    "print(f\"Data: {X.shape}\")\n",
    "print(f\"Labels: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test(\n",
    "    X, y,\n",
    "    classifier='lda',\n",
    "    n_permutations=1000,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Run permutation test for classification significance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    observed : float\n",
    "        Observed accuracy\n",
    "    p_value : float\n",
    "        P-value from permutation test\n",
    "    null_distribution : np.ndarray\n",
    "        Null distribution of accuracies\n",
    "    \"\"\"\n",
    "    # Create classifier\n",
    "    if classifier == 'lda':\n",
    "        clf = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            LDA(solver='lsqr', shrinkage='auto')\n",
    "        )\n",
    "    elif classifier == 'svm':\n",
    "        clf = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SVC(kernel='linear')\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown classifier: {classifier}\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_obj = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Permutation test\n",
    "    observed, null_dist, p_value = permutation_test_score(\n",
    "        clf, X, y,\n",
    "        cv=cv_obj,\n",
    "        n_permutations=n_permutations,\n",
    "        n_jobs=n_jobs,\n",
    "        random_state=random_state,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    return observed, p_value, null_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run permutation test\n",
    "observed, p_value, null_dist = permutation_test(\n",
    "    X, y,\n",
    "    classifier='lda',\n",
    "    n_permutations=500,  # Use 1000+ for publication\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "print(\"Permutation Test Results:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Observed accuracy: {observed:.1%}\")\n",
    "print(f\"Null mean: {np.mean(null_dist):.1%}\")\n",
    "print(f\"Null std: {np.std(null_dist):.1%}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"Significant (p<0.05): {p_value < 0.05}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot null distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Histogram of null distribution\n",
    "ax.hist(null_dist, bins=50, color='gray', alpha=0.7, \n",
    "        edgecolor='black', label='Null distribution')\n",
    "\n",
    "# Observed accuracy\n",
    "ax.axvline(observed, color='red', linewidth=3, linestyle='--',\n",
    "           label=f'Observed: {observed:.1%}')\n",
    "\n",
    "# Chance level\n",
    "ax.axvline(0.5, color='black', linewidth=2, linestyle=':',\n",
    "           label='Chance: 50%')\n",
    "\n",
    "# 95th percentile of null\n",
    "threshold_95 = np.percentile(null_dist, 95)\n",
    "ax.axvline(threshold_95, color='orange', linewidth=2, linestyle='-',\n",
    "           label=f'95th percentile: {threshold_95:.1%}')\n",
    "\n",
    "ax.set_xlabel('Accuracy', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title(f'Permutation Test (p = {p_value:.4f})', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Group-Level T-Tests\n",
    "\n",
    "Test if group mean accuracy is significantly above chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multi-subject accuracies\n",
    "n_subjects = 20\n",
    "\n",
    "# Simulated subject accuracies (with effect)\n",
    "np.random.seed(42)\n",
    "subject_accuracies = 0.5 + 0.15 * np.random.randn(n_subjects) + 0.1  # Above chance\n",
    "subject_accuracies = np.clip(subject_accuracies, 0.3, 0.9)  # Realistic bounds\n",
    "\n",
    "print(f\"Subject accuracies: N={n_subjects}\")\n",
    "print(f\"Mean: {np.mean(subject_accuracies):.1%}\")\n",
    "print(f\"Std: {np.std(subject_accuracies):.1%}\")\n",
    "print(f\"Range: [{np.min(subject_accuracies):.1%}, {np.max(subject_accuracies):.1%}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_ttest(accuracies, chance_level=0.5, alternative='greater'):\n",
    "    \"\"\"\n",
    "    One-sample t-test against chance level.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    accuracies : np.ndarray\n",
    "        Subject accuracies\n",
    "    chance_level : float\n",
    "        Chance level (0.5 for binary)\n",
    "    alternative : str\n",
    "        'two-sided', 'greater', or 'less'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with t-statistic, p-value, effect size (Cohen's d)\n",
    "    \"\"\"\n",
    "    n = len(accuracies)\n",
    "    mean = np.mean(accuracies)\n",
    "    std = np.std(accuracies, ddof=1)\n",
    "    \n",
    "    # T-test\n",
    "    t_stat, p_value = stats.ttest_1samp(accuracies, chance_level)\n",
    "    \n",
    "    # Adjust for one-tailed if needed\n",
    "    if alternative == 'greater':\n",
    "        p_value = p_value / 2 if t_stat > 0 else 1 - p_value / 2\n",
    "    elif alternative == 'less':\n",
    "        p_value = p_value / 2 if t_stat < 0 else 1 - p_value / 2\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    cohens_d = (mean - chance_level) / std\n",
    "    \n",
    "    # Confidence interval\n",
    "    se = std / np.sqrt(n)\n",
    "    ci_95 = stats.t.interval(0.95, df=n-1, loc=mean, scale=se)\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'n': n,\n",
    "        't_stat': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'df': n - 1,\n",
    "        'cohens_d': cohens_d,\n",
    "        'ci_95': ci_95\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run group t-test\n",
    "result = group_ttest(subject_accuracies, chance_level=0.5, alternative='greater')\n",
    "\n",
    "print(\"Group-Level T-Test (vs. 50% chance):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean accuracy: {result['mean']:.1%} ± {result['std']:.1%}\")\n",
    "print(f\"t({result['df']}) = {result['t_stat']:.3f}\")\n",
    "print(f\"p-value (one-tailed): {result['p_value']:.4f}\")\n",
    "print(f\"Cohen's d: {result['cohens_d']:.2f}\")\n",
    "print(f\"95% CI: [{result['ci_95'][0]:.1%}, {result['ci_95'][1]:.1%}]\")\n",
    "print(f\"\\nSignificant (p<0.05): {result['p_value'] < 0.05}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-parametric alternative: Wilcoxon signed-rank test\n",
    "w_stat, w_pvalue = stats.wilcoxon(subject_accuracies - 0.5, alternative='greater')\n",
    "\n",
    "print(\"\\nWilcoxon Signed-Rank Test (non-parametric):\")\n",
    "print(f\"W = {w_stat:.1f}\")\n",
    "print(f\"p-value: {w_pvalue:.4f}\")\n",
    "print(f\"Significant (p<0.05): {w_pvalue < 0.05}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot with individual subjects\n",
    "ax = axes[0]\n",
    "x = np.arange(n_subjects) + 1\n",
    "ax.bar(x, subject_accuracies, color='steelblue', edgecolor='black')\n",
    "ax.axhline(0.5, color='red', linestyle='--', linewidth=2, label='Chance')\n",
    "ax.axhline(result['mean'], color='green', linestyle='-', linewidth=2, \n",
    "           label=f\"Mean: {result['mean']:.1%}\")\n",
    "ax.set_xlabel('Subject', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Per-Subject Classification Accuracy', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "# Box plot with stats\n",
    "ax = axes[1]\n",
    "bp = ax.boxplot(subject_accuracies, vert=True, patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('steelblue')\n",
    "ax.scatter(np.ones(n_subjects), subject_accuracies, alpha=0.6, color='black', s=50)\n",
    "ax.axhline(0.5, color='red', linestyle='--', linewidth=2, label='Chance')\n",
    "\n",
    "# Add stats annotation\n",
    "stats_text = f\"t({result['df']}) = {result['t_stat']:.2f}\\np = {result['p_value']:.4f}\\nd = {result['cohens_d']:.2f}\"\n",
    "ax.text(1.3, result['mean'], stats_text, fontsize=11, verticalalignment='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Group Distribution', fontsize=14)\n",
    "ax.set_xticks([1])\n",
    "ax.set_xticklabels(['All Subjects'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Multiple Comparison Correction\n",
    "\n",
    "When testing multiple ROIs, correct for multiple comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate ROI-level results (426 ROIs, like HCP atlas)\n",
    "n_rois = 426\n",
    "np.random.seed(42)\n",
    "\n",
    "# Most ROIs are at chance, some have signal\n",
    "roi_accuracies = 0.5 + 0.03 * np.random.randn(n_rois)  # Mostly noise\n",
    "\n",
    "# Add signal to some ROIs\n",
    "signal_rois = [10, 25, 50, 100, 150, 200, 250, 300]\n",
    "roi_accuracies[signal_rois] += 0.15\n",
    "\n",
    "# Simulate subject data for each ROI (for t-tests)\n",
    "n_subjects = 15\n",
    "roi_subject_accs = np.zeros((n_rois, n_subjects))\n",
    "for r in range(n_rois):\n",
    "    roi_subject_accs[r] = roi_accuracies[r] + 0.08 * np.random.randn(n_subjects)\n",
    "\n",
    "print(f\"Testing {n_rois} ROIs\")\n",
    "print(f\"Signal ROIs: {signal_rois}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roi_significance_testing(roi_subject_accs, chance_level=0.5, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Test each ROI and apply multiple comparison correction.\n",
    "    \n",
    "    Returns dict with uncorrected and corrected results.\n",
    "    \"\"\"\n",
    "    n_rois = roi_subject_accs.shape[0]\n",
    "    \n",
    "    # T-tests for each ROI\n",
    "    p_values = []\n",
    "    t_stats = []\n",
    "    \n",
    "    for r in range(n_rois):\n",
    "        t, p = stats.ttest_1samp(roi_subject_accs[r], chance_level)\n",
    "        # One-tailed\n",
    "        p = p / 2 if t > 0 else 1 - p / 2\n",
    "        p_values.append(p)\n",
    "        t_stats.append(t)\n",
    "    \n",
    "    p_values = np.array(p_values)\n",
    "    t_stats = np.array(t_stats)\n",
    "    \n",
    "    # Multiple comparison corrections\n",
    "    \n",
    "    # Bonferroni\n",
    "    _, p_bonf, _, _ = multipletests(p_values, alpha=alpha, method='bonferroni')\n",
    "    sig_bonf = p_bonf < alpha\n",
    "    \n",
    "    # FDR (Benjamini-Hochberg)\n",
    "    _, p_fdr, _, _ = multipletests(p_values, alpha=alpha, method='fdr_bh')\n",
    "    sig_fdr = p_fdr < alpha\n",
    "    \n",
    "    # Uncorrected\n",
    "    sig_uncorr = p_values < alpha\n",
    "    \n",
    "    return {\n",
    "        'p_values': p_values,\n",
    "        't_stats': t_stats,\n",
    "        'p_bonferroni': p_bonf,\n",
    "        'p_fdr': p_fdr,\n",
    "        'sig_uncorrected': sig_uncorr,\n",
    "        'sig_bonferroni': sig_bonf,\n",
    "        'sig_fdr': sig_fdr,\n",
    "        'n_sig_uncorrected': np.sum(sig_uncorr),\n",
    "        'n_sig_bonferroni': np.sum(sig_bonf),\n",
    "        'n_sig_fdr': np.sum(sig_fdr)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple comparison testing\n",
    "mcc_results = roi_significance_testing(roi_subject_accs, chance_level=0.5, alpha=0.05)\n",
    "\n",
    "print(\"Multiple Comparison Correction Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total ROIs tested: {n_rois}\")\n",
    "print(f\"\\nSignificant ROIs:\")\n",
    "print(f\"  Uncorrected (p<0.05): {mcc_results['n_sig_uncorrected']}\")\n",
    "print(f\"  Bonferroni corrected: {mcc_results['n_sig_bonferroni']}\")\n",
    "print(f\"  FDR corrected (BH):   {mcc_results['n_sig_fdr']}\")\n",
    "\n",
    "# Check if true signal ROIs were detected\n",
    "print(f\"\\nTrue signal ROIs: {signal_rois}\")\n",
    "detected_fdr = np.where(mcc_results['sig_fdr'])[0]\n",
    "print(f\"Detected (FDR): {list(detected_fdr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# P-value distribution\n",
    "ax = axes[0]\n",
    "ax.hist(mcc_results['p_values'], bins=50, color='steelblue', edgecolor='black')\n",
    "ax.axvline(0.05, color='red', linestyle='--', linewidth=2, label='α = 0.05')\n",
    "ax.set_xlabel('P-value (uncorrected)', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('P-value Distribution', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "# Sorted p-values with FDR threshold\n",
    "ax = axes[1]\n",
    "sorted_idx = np.argsort(mcc_results['p_values'])\n",
    "sorted_p = mcc_results['p_values'][sorted_idx]\n",
    "ranks = np.arange(1, n_rois + 1)\n",
    "fdr_threshold = 0.05 * ranks / n_rois  # BH threshold line\n",
    "\n",
    "ax.plot(ranks[:100], sorted_p[:100], 'b-', linewidth=2, label='Sorted p-values')\n",
    "ax.plot(ranks[:100], fdr_threshold[:100], 'r--', linewidth=2, label='FDR threshold')\n",
    "ax.set_xlabel('Rank', fontsize=12)\n",
    "ax.set_ylabel('P-value', fontsize=12)\n",
    "ax.set_title('FDR (Benjamini-Hochberg)', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "# ROI significance map\n",
    "ax = axes[2]\n",
    "x = np.arange(n_rois)\n",
    "ax.bar(x, roi_subject_accs.mean(axis=1), color='lightgray', width=1)\n",
    "# Highlight significant ROIs\n",
    "sig_mask = mcc_results['sig_fdr']\n",
    "ax.bar(x[sig_mask], roi_subject_accs.mean(axis=1)[sig_mask], \n",
    "       color='red', width=1, label=f'FDR sig (n={np.sum(sig_mask)})')\n",
    "ax.axhline(0.5, color='black', linestyle='--', linewidth=1)\n",
    "ax.set_xlabel('ROI Index', fontsize=12)\n",
    "ax.set_ylabel('Mean Accuracy', fontsize=12)\n",
    "ax.set_title('ROI Accuracies (FDR significant in red)', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Cluster-Based Permutation for Temporal Decoding\n",
    "\n",
    "Test significance of temporal decoding while correcting for multiple time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate temporal decoding results\n",
    "n_subjects = 15\n",
    "n_times = 100\n",
    "times = np.linspace(-0.2, 0.8, n_times)\n",
    "\n",
    "# Subject x time accuracy matrix\n",
    "np.random.seed(42)\n",
    "temporal_accs = 0.5 + 0.02 * np.random.randn(n_subjects, n_times)\n",
    "\n",
    "# Add signal in REWP window (240-340ms)\n",
    "rewp_mask = (times >= 0.24) & (times <= 0.34)\n",
    "temporal_accs[:, rewp_mask] += 0.12 + 0.03 * np.random.randn(n_subjects, np.sum(rewp_mask))\n",
    "\n",
    "print(f\"Temporal data: {n_subjects} subjects x {n_times} time points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_permutation_test(\n",
    "    subject_scores,\n",
    "    times,\n",
    "    chance_level=0.5,\n",
    "    n_permutations=1000,\n",
    "    cluster_alpha=0.05,\n",
    "    tail=1  # 1 for greater, -1 for less, 0 for two-sided\n",
    "):\n",
    "    \"\"\"\n",
    "    Cluster-based permutation test for temporal decoding.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_scores : np.ndarray\n",
    "        (n_subjects, n_times) accuracy matrix\n",
    "    times : np.ndarray\n",
    "        Time vector\n",
    "    chance_level : float\n",
    "        Chance level\n",
    "    n_permutations : int\n",
    "        Number of permutations\n",
    "    cluster_alpha : float\n",
    "        Threshold for cluster formation\n",
    "    tail : int\n",
    "        1 (greater), -1 (less), 0 (two-sided)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with clusters, p-values, etc.\n",
    "    \"\"\"\n",
    "    n_subjects, n_times = subject_scores.shape\n",
    "    \n",
    "    # T-test at each time point\n",
    "    t_stats, _ = stats.ttest_1samp(subject_scores, chance_level, axis=0)\n",
    "    \n",
    "    # Threshold for cluster formation\n",
    "    t_threshold = stats.t.ppf(1 - cluster_alpha, df=n_subjects - 1)\n",
    "    \n",
    "    if tail == 1:\n",
    "        cluster_mask = t_stats > t_threshold\n",
    "    elif tail == -1:\n",
    "        cluster_mask = t_stats < -t_threshold\n",
    "    else:\n",
    "        cluster_mask = np.abs(t_stats) > t_threshold\n",
    "    \n",
    "    # Find clusters\n",
    "    labeled_array, n_clusters = scipy_label(cluster_mask)\n",
    "    \n",
    "    # Compute cluster statistics (sum of t-values in cluster)\n",
    "    observed_cluster_stats = []\n",
    "    cluster_info = []\n",
    "    \n",
    "    for i in range(1, n_clusters + 1):\n",
    "        cluster_indices = labeled_array == i\n",
    "        cluster_t_sum = np.sum(t_stats[cluster_indices])\n",
    "        observed_cluster_stats.append(cluster_t_sum)\n",
    "        \n",
    "        cluster_times = times[cluster_indices]\n",
    "        cluster_info.append({\n",
    "            'cluster_id': i,\n",
    "            'start': cluster_times[0],\n",
    "            'end': cluster_times[-1],\n",
    "            't_sum': cluster_t_sum,\n",
    "            'indices': cluster_indices\n",
    "        })\n",
    "    \n",
    "    # Permutation testing\n",
    "    max_cluster_stats = []\n",
    "    \n",
    "    for perm in range(n_permutations):\n",
    "        # Randomly flip signs (equivalent to permuting condition labels)\n",
    "        signs = np.random.choice([-1, 1], size=n_subjects)\n",
    "        perm_scores = subject_scores.copy()\n",
    "        perm_scores = (perm_scores - chance_level) * signs[:, np.newaxis] + chance_level\n",
    "        \n",
    "        # T-test\n",
    "        perm_t, _ = stats.ttest_1samp(perm_scores, chance_level, axis=0)\n",
    "        \n",
    "        # Find clusters\n",
    "        if tail == 1:\n",
    "            perm_mask = perm_t > t_threshold\n",
    "        elif tail == -1:\n",
    "            perm_mask = perm_t < -t_threshold\n",
    "        else:\n",
    "            perm_mask = np.abs(perm_t) > t_threshold\n",
    "        \n",
    "        perm_labeled, perm_n_clusters = scipy_label(perm_mask)\n",
    "        \n",
    "        # Get max cluster stat\n",
    "        if perm_n_clusters > 0:\n",
    "            perm_cluster_stats = [\n",
    "                np.sum(perm_t[perm_labeled == j]) \n",
    "                for j in range(1, perm_n_clusters + 1)\n",
    "            ]\n",
    "            max_cluster_stats.append(np.max(np.abs(perm_cluster_stats)))\n",
    "        else:\n",
    "            max_cluster_stats.append(0)\n",
    "    \n",
    "    max_cluster_stats = np.array(max_cluster_stats)\n",
    "    \n",
    "    # Compute cluster p-values\n",
    "    for info in cluster_info:\n",
    "        p = np.mean(max_cluster_stats >= np.abs(info['t_sum']))\n",
    "        info['p_value'] = p\n",
    "        info['significant'] = p < 0.05\n",
    "    \n",
    "    return {\n",
    "        't_stats': t_stats,\n",
    "        't_threshold': t_threshold,\n",
    "        'clusters': cluster_info,\n",
    "        'n_clusters': n_clusters,\n",
    "        'max_cluster_dist': max_cluster_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cluster permutation test\n",
    "cluster_results = cluster_permutation_test(\n",
    "    temporal_accs,\n",
    "    times,\n",
    "    chance_level=0.5,\n",
    "    n_permutations=500,  # Use 1000+ for publication\n",
    "    cluster_alpha=0.05\n",
    ")\n",
    "\n",
    "print(\"Cluster-Based Permutation Test:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"T-threshold: {cluster_results['t_threshold']:.3f}\")\n",
    "print(f\"Number of clusters found: {cluster_results['n_clusters']}\")\n",
    "print()\n",
    "\n",
    "for cluster in cluster_results['clusters']:\n",
    "    sig_str = \"*\" if cluster['significant'] else \"\"\n",
    "    print(f\"Cluster {cluster['cluster_id']}: {cluster['start']*1000:.0f}-{cluster['end']*1000:.0f}ms\")\n",
    "    print(f\"  t-sum = {cluster['t_sum']:.2f}\")\n",
    "    print(f\"  p = {cluster['p_value']:.4f} {sig_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Mean temporal decoding with CI\n",
    "ax = axes[0]\n",
    "mean_scores = temporal_accs.mean(axis=0)\n",
    "sem = temporal_accs.std(axis=0) / np.sqrt(n_subjects)\n",
    "\n",
    "ax.plot(times * 1000, mean_scores, 'b-', linewidth=2, label='Mean accuracy')\n",
    "ax.fill_between(times * 1000, mean_scores - sem, mean_scores + sem, alpha=0.3)\n",
    "ax.axhline(0.5, color='gray', linestyle='--', linewidth=1, label='Chance')\n",
    "ax.axvline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Highlight significant clusters\n",
    "for cluster in cluster_results['clusters']:\n",
    "    if cluster['significant']:\n",
    "        ax.axvspan(cluster['start']*1000, cluster['end']*1000, \n",
    "                   alpha=0.3, color='green', label=f\"p={cluster['p_value']:.3f}\")\n",
    "\n",
    "ax.set_xlabel('Time (ms)', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Temporal Decoding with Cluster Correction', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "# T-statistics\n",
    "ax = axes[1]\n",
    "ax.plot(times * 1000, cluster_results['t_stats'], 'b-', linewidth=2)\n",
    "ax.axhline(cluster_results['t_threshold'], color='red', linestyle='--', \n",
    "           linewidth=1, label=f\"Threshold: t={cluster_results['t_threshold']:.2f}\")\n",
    "ax.axhline(-cluster_results['t_threshold'], color='red', linestyle='--', linewidth=1)\n",
    "ax.axhline(0, color='gray', linestyle='-', linewidth=0.5)\n",
    "ax.axvline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Time (ms)', fontsize=12)\n",
    "ax.set_ylabel('t-statistic', fontsize=12)\n",
    "ax.set_title('T-statistics Across Time', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Bootstrap Confidence Intervals\n",
    "\n",
    "Estimate uncertainty in accuracy estimates using bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(accuracies, n_bootstrap=10000, ci=95, random_state=42):\n",
    "    \"\"\"\n",
    "    Compute bootstrap confidence interval for mean accuracy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    accuracies : np.ndarray\n",
    "        Subject accuracies\n",
    "    n_bootstrap : int\n",
    "        Number of bootstrap samples\n",
    "    ci : float\n",
    "        Confidence level (e.g., 95)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with mean, CI bounds, bootstrap distribution\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    n = len(accuracies)\n",
    "    \n",
    "    # Bootstrap resampling\n",
    "    boot_means = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        boot_sample = np.random.choice(accuracies, size=n, replace=True)\n",
    "        boot_means.append(np.mean(boot_sample))\n",
    "    \n",
    "    boot_means = np.array(boot_means)\n",
    "    \n",
    "    # Percentile method\n",
    "    alpha = (100 - ci) / 2\n",
    "    ci_lower = np.percentile(boot_means, alpha)\n",
    "    ci_upper = np.percentile(boot_means, 100 - alpha)\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(accuracies),\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'ci': ci,\n",
    "        'boot_distribution': boot_means\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bootstrap CI\n",
    "boot_result = bootstrap_ci(subject_accuracies, n_bootstrap=10000, ci=95)\n",
    "\n",
    "print(\"Bootstrap Confidence Interval:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Mean accuracy: {boot_result['mean']:.1%}\")\n",
    "print(f\"95% CI: [{boot_result['ci_lower']:.1%}, {boot_result['ci_upper']:.1%}]\")\n",
    "\n",
    "# Check if CI excludes chance\n",
    "excludes_chance = boot_result['ci_lower'] > 0.5\n",
    "print(f\"\\n95% CI excludes chance (50%): {excludes_chance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.hist(boot_result['boot_distribution'], bins=50, color='steelblue', \n",
    "        alpha=0.7, edgecolor='black')\n",
    "ax.axvline(boot_result['mean'], color='green', linewidth=3, \n",
    "           label=f\"Mean: {boot_result['mean']:.1%}\")\n",
    "ax.axvline(boot_result['ci_lower'], color='orange', linewidth=2, linestyle='--',\n",
    "           label=f\"95% CI: [{boot_result['ci_lower']:.1%}, {boot_result['ci_upper']:.1%}]\")\n",
    "ax.axvline(boot_result['ci_upper'], color='orange', linewidth=2, linestyle='--')\n",
    "ax.axvline(0.5, color='red', linewidth=2, linestyle=':', label='Chance')\n",
    "\n",
    "ax.set_xlabel('Mean Accuracy', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('Bootstrap Distribution', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Publication-Ready Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stats_summary(accuracies, chance_level=0.5):\n",
    "    \"\"\"\n",
    "    Create publication-ready statistics summary.\n",
    "    \"\"\"\n",
    "    # T-test\n",
    "    ttest_result = group_ttest(accuracies, chance_level, alternative='greater')\n",
    "    \n",
    "    # Wilcoxon\n",
    "    w_stat, w_p = stats.wilcoxon(accuracies - chance_level, alternative='greater')\n",
    "    \n",
    "    # Bootstrap\n",
    "    boot_result = bootstrap_ci(accuracies)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STATISTICAL SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nDescriptive Statistics (N={len(accuracies)}):\")\n",
    "    print(f\"  Mean ± SD: {ttest_result['mean']:.1%} ± {ttest_result['std']:.1%}\")\n",
    "    print(f\"  Range: [{np.min(accuracies):.1%}, {np.max(accuracies):.1%}]\")\n",
    "    print(f\"  95% CI: [{boot_result['ci_lower']:.1%}, {boot_result['ci_upper']:.1%}]\")\n",
    "    \n",
    "    print(f\"\\nParametric Test (One-sample t-test vs {chance_level:.0%}):\")\n",
    "    print(f\"  t({ttest_result['df']}) = {ttest_result['t_stat']:.3f}, p = {ttest_result['p_value']:.4f}\")\n",
    "    print(f\"  Cohen's d = {ttest_result['cohens_d']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nNon-parametric Test (Wilcoxon signed-rank):\")\n",
    "    print(f\"  W = {w_stat:.1f}, p = {w_p:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # APA-style string\n",
    "    apa_string = f\"M = {ttest_result['mean']:.1%}, SD = {ttest_result['std']:.1%}, \"\n",
    "    apa_string += f\"t({ttest_result['df']}) = {ttest_result['t_stat']:.2f}, \"\n",
    "    apa_string += f\"p = {ttest_result['p_value']:.3f}, d = {ttest_result['cohens_d']:.2f}\"\n",
    "    print(f\"\\nAPA-style: {apa_string}\")\n",
    "\n",
    "# Run summary\n",
    "create_stats_summary(subject_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook covered:\n",
    "\n",
    "1. **Single-subject permutation testing**: Compare observed accuracy to null distribution\n",
    "2. **Group-level t-tests**: One-sample t-test vs. chance with effect size\n",
    "3. **Multiple comparison correction**: Bonferroni and FDR for many ROIs\n",
    "4. **Cluster-based permutation**: Correct for multiple time points\n",
    "5. **Bootstrap confidence intervals**: Non-parametric uncertainty estimation\n",
    "\n",
    "### Key Functions:\n",
    "- `permutation_test()`: Single-subject significance\n",
    "- `group_ttest()`: Group-level inference\n",
    "- `multipletests()`: FDR/Bonferroni correction\n",
    "- `cluster_permutation_test()`: Temporal cluster correction\n",
    "- `bootstrap_ci()`: Confidence intervals\n",
    "\n",
    "### Recommended Usage:\n",
    "- **Single subject**: Permutation test (≥1000 permutations)\n",
    "- **Group level**: One-sample t-test + Cohen's d\n",
    "- **Many ROIs**: FDR correction (less conservative than Bonferroni)\n",
    "- **Temporal data**: Cluster-based permutation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
